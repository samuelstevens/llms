# Resources

Might want to use batches with similar lengths:

* https://github.com/jonathanking/sidechainnet/blob/4d4f57204c162ab938b8762dfacffb1d992774d0/sidechainnet/dataloaders/SimilarLengthBatchSampler.py#L9

Next steps:

1. NestedTensors for faster batched inference
2. Fully Sharded Data Parallel

Eventual goals:

1. Instruction-tuning Llama
2. Toolformer + Llama
